<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Mehmet Sat | Personal Blog</title><link>https://mehmetsat.github.io/</link><description>Recent content on Mehmet Sat | Personal Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 03 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://mehmetsat.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Virtual Private Clouds(VPC)</title><link>https://mehmetsat.github.io/homepage/aws-notes-vpc/</link><pubDate>Wed, 03 Aug 2022 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/aws-notes-vpc/</guid><description>Virtual Private Clouds (VPC) You can think of VPCs as a logical data center in AWS It consists of : Internet or Virtual Private Gateways Route Tables Network Access Control Lists Subnets Security Groups An Example of VPC Architecture Here you can see a custom VPC created in us-east-1 region with the CIDR Block Range of 10.0.0.0/16 ðŸ’¡ You can think of a CIDR block as a group of IP Addresses that has the same prefix up to some bits.</description></item><item><title>AWS Databases</title><link>https://mehmetsat.github.io/homepage/aws-notes-databases/</link><pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/aws-notes-databases/</guid><description>AWS Databases RDS(Relational Database Service) 6 different types : SQL Server Oracle MySQL PostgreSQL MariaDB Amazon Aurora RDS is generally used for OLTP(online transaction processing) workloads Great for processing lots of small transactions like customer orders, banking transactions, payments, and booking systems It is not suitable for OLAP(online analytics processing) workloads. There are better alternatives like Amazon Redshift for OLAP. Read Replicas It is good for scaling the read performance, not disaster recovery!</description></item><item><title>Amazon EBS</title><link>https://mehmetsat.github.io/homepage/aws-notes-ebs/</link><pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/aws-notes-ebs/</guid><description>Amazon EBS Highly available and scalable persistent storage volumes that you can attach to your EC2 instances EBS Types SSDs General Purpose SSDs
gp2: Suitable for boot disks and general applications Up to 16000 IOPS per volume Up to 99.9% durability gp3: Suitable for high performance applications Predictable 3000 IOPS baseline performance 125 MiB/s regardless of volume size Up to 99.9% durability Provisioned IOPS SSDs
io1: Suitable for OLTP(online transaction processing) and latency sensitive applications 50IOPS/GiB Up to 64000 IOPS per volume High performance and most expensive Up to 99.</description></item><item><title>Amazon EC2</title><link>https://mehmetsat.github.io/homepage/aws-notes-ec2/</link><pubDate>Wed, 27 Jul 2022 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/aws-notes-ec2/</guid><description>EC2 is like a VM, hosted in AWS instead of your own data center Select the capacity you need right now. Grow and shrink when you need Pay for what you use EC2 types and Pricing options On-Demand Instances
Pay by the hour or the second Great for flexibility, but expensive Spot Instances
Purchase unused capacity at a discount of up to %90 Prices fluctuate with supply and demand Great for applications with flexible start and times Reserved Instances</description></item><item><title>Amazon S3</title><link>https://mehmetsat.github.io/homepage/aws-notes-s3/</link><pubDate>Tue, 26 Jul 2022 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/aws-notes-s3/</guid><description>Object-Based Storage that allows you to upload files
Files can be from 0b to 5TB
Not suitable for installing an operating system or a running a database
The total volume of the data and the number of the objects are unlimited.
S3 is a universal namespace which means that cannot exist 2 files with the same name.
Example
https://**{bucket-name}**.s3.**{Region}**.amazonaws.com/**{key-name} # succesful CLI or API uploads will response HTTP 200 status code** Four s3 Object Tips Key â†’ object-name Value â†’ the data itself Version ID â†’ allows multiple versions Metadata â†’ Data about the data you are storing e.</description></item><item><title>AWS IAM</title><link>https://mehmetsat.github.io/homepage/aws-notes-iam/</link><pubDate>Mon, 25 Jul 2022 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/aws-notes-iam/</guid><description>IAM 4 Steps to Secure Your AWS Root Account:
Enable multi-factor authentication(MFA) on the root account Create an admin group for your administrators, and assign the appropriate permissions to this group Create user accounts for your administrators Add your users to the admin group Study the IAM Policy Documents
Example: { &amp;#34;version&amp;#34; : &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34; : [ { &amp;#34;Effect&amp;#34; : &amp;#34;Allow&amp;#34;, &amp;#34;Action&amp;#34; : &amp;#34;*&amp;#34;, &amp;#34;Resource&amp;#34; : &amp;#34;*&amp;#34; } ] } IAM is Universal: It does not differ for the regions</description></item><item><title>AWS Fundamentals</title><link>https://mehmetsat.github.io/homepage/aws-notes-aws-fundamentals/</link><pubDate>Sun, 24 Jul 2022 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/aws-notes-aws-fundamentals/</guid><description>AWS Fundamentals A Region is a physical location in the world that consists of two or more Availability Zones(AZs) An AZ is one or more discrete data centers â€” each with redundant power, networking, and connectivity â€” housed in separate facilities Edge Locations are endpoints for AWS that are used for caching content. Typically, this consists of CloudFront, Amazonâ€™s Content Delivery Network. Edge locations is more than Availability Zones Shared Responsibility Model: Ask this question for the answer if you are responsible or not Can you do this yourself in the AWS Management Console?</description></item><item><title>Guided Super Resolution as Pixel-to-Pixel Transformation</title><link>https://mehmetsat.github.io/homepage/guided-super-resolution-as-pixel-to-pixel-transformation/</link><pubDate>Fri, 08 Jul 2022 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/guided-super-resolution-as-pixel-to-pixel-transformation/</guid><description>Guided Super Resolution as Pixel-to-Pixel Transformation Paper Review In this blog post I will try to explain what I have understand from the paper Guided Super Resolution as Pixel-to-Pixel Transformation which I hope it helps for who is interested. As a matter of fact, all the information is gathered from the paper itself otherwise it will be indicated.
First letâ€™s start with some definition of the problem. The task of Guided Super Resolution can be defined as a unifying framework for several computer vision tasks where the inputs are</description></item><item><title>AWS Personalize Tutorial</title><link>https://mehmetsat.github.io/homepage/aws-personalize-tutorial/</link><pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/aws-personalize-tutorial/</guid><description>INTRODUCTION AWS Personalize dashboard summarizes all the steps for deploying the personalize model.
Creating Dataset Group and uploading datasets
SDK Installation(Optional) for live user event tracking
Creating solutions
Launching campaigns for getting recommendations for users
USEFUL LINKS github.com/aws-samples/amazon-personalize-samples
AWS Personalize Documentation
DATASET PREPARATION There are 3 types of datasets:
User-item interaction dataset
User Metadata dataset
Item Metadata dataset
User-Item Interactions Dataset A schema for the dataset must be defined in advance.</description></item><item><title>Data Wrangling with PySpark</title><link>https://mehmetsat.github.io/homepage/data-wrangling-with-pyspark/</link><pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/data-wrangling-with-pyspark/</guid><description>It becomes very hard to work with pandas when the data is huge. Instead as a strong alternative PySpark comes along. In this tutorial I will introduce the PySpark and mirror our pandas abilities to PySpark. This post assumes the reader has familiarity with the pandas library.
Introduction to Spark The first question is what is Spark. Apache Spark is fast and general engine for large-scale data processing. It distributes computations among the CPU cores to do parallel processing.</description></item><item><title>Recommendation Engines Overview</title><link>https://mehmetsat.github.io/homepage/recommendation-engines-overview/</link><pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/recommendation-engines-overview/</guid><description>In these days, for customers there are so many alternatives to use a mobile app for a specific task and most of them work well in terms of functionality. What differs in terms of the customer is the experience you gave them. The most powerful influence that you can give to the customer is making them feel special. To do this you must provide a personalized experience for each customer. Recommendation Engines are exactly developed on that purpose.</description></item><item><title>About me</title><link>https://mehmetsat.github.io/homepage/about/</link><pubDate>Mon, 28 Jun 2021 00:00:00 +0000</pubDate><guid>https://mehmetsat.github.io/homepage/about/</guid><description>Even in my childhood, I was always known as the tech person that solves computer related problems in my village. I was fascinated by the improvements of the computer and mobile phone industry, and keeping track of every new development in these industries over the years. This leads to choosing Electrical and Electronics Engineering as the field of study of my major.
As the source of my self confidence, I dreamt about being accepted to the best university in that field in Turkey which is Bogazici University and achieved this first big success in my educational path.</description></item></channel></rss>