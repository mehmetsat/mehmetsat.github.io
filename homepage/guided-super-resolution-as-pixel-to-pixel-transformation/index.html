<!doctype html><html class="not-ready lg:text-base" style=--bg:#faf8f1 lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Guided Super Resolution as Pixel-to-Pixel Transformation - Mehmet Sat | Personal Blog</title><meta name=theme-color><meta name=description content="Guided Super Resolution as Pixel-to-Pixel Transformation Paper Review In this blog post I will try to explain what I have understand from the paper Guided Super Resolution as Pixel-to-Pixel Transformation which I hope it helps for who is interested. As a matter of fact, all the information is gathered from the paper itself otherwise it will be indicated.
First let’s start with some definition of the problem. The task of Guided Super Resolution can be defined as a unifying framework for several computer vision tasks where the inputs are"><meta name=author content="mehmet sat"><link rel="preload stylesheet" as=style href=https://mehmetsat.github.io/main.min.css><link rel=preload as=image href=https://mehmetsat.github.io/theme.png><script defer src=https://mehmetsat.github.io/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",()=>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1}))</script><link rel=icon href=https://mehmetsat.github.io/favicon.ico><link rel=apple-touch-icon href=https://mehmetsat.github.io/apple-touch-icon.png><meta name=generator content="Hugo 0.119.0"><meta itemprop=name content="Guided Super Resolution as Pixel-to-Pixel Transformation"><meta itemprop=description content="Guided Super Resolution as Pixel-to-Pixel Transformation Paper Review"><meta itemprop=datePublished content="2022-07-08T00:00:00+00:00"><meta itemprop=dateModified content="2022-07-08T00:00:00+00:00"><meta itemprop=wordCount content="1322"><meta itemprop=keywords content><meta property="og:title" content="Guided Super Resolution as Pixel-to-Pixel Transformation"><meta property="og:description" content="Guided Super Resolution as Pixel-to-Pixel Transformation Paper Review"><meta property="og:type" content="article"><meta property="og:url" content="https://mehmetsat.github.io/homepage/guided-super-resolution-as-pixel-to-pixel-transformation/"><meta property="article:section" content="homepage"><meta property="article:published_time" content="2022-07-08T00:00:00+00:00"><meta property="article:modified_time" content="2022-07-08T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Guided Super Resolution as Pixel-to-Pixel Transformation"><meta name=twitter:description content="Guided Super Resolution as Pixel-to-Pixel Transformation Paper Review"><link rel=canonical href=https://mehmetsat.github.io/homepage/guided-super-resolution-as-pixel-to-pixel-transformation/></head><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center"><div class="relative z-50 mr-auto flex items-center"><a class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold" href=https://mehmetsat.github.io/>Mehmet Sat | Personal Blog</a><div class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#faf8f1".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6"><a class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal" href=/about/>About</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"><article><header class=mb-16><h1 class="!my-0 pb-2.5">Guided Super Resolution as Pixel-to-Pixel Transformation</h1><div class="text-sm antialiased opacity-60"><time>Jul 8, 2022</time>
<span class=mx-1>&#183;</span>
<span>mehmet sat</span></div></header><section><h1 id=guided-super-resolution-as-pixel-to-pixel-transformation-paper-review>Guided Super Resolution as Pixel-to-Pixel Transformation Paper Review</h1><p>In this blog post I will try to explain what I have understand from the paper Guided Super Resolution as Pixel-to-Pixel Transformation which I hope it helps for who is interested. As a matter of fact, all the information is gathered from the paper itself otherwise it will be indicated.</p><p>First let’s start with some definition of the problem. The task of Guided Super Resolution can be defined as a unifying framework for several computer vision tasks where the inputs are</p><ul><li>a low resolution source image of some target quantity</li><li>a high resolution guide image from different domain
and the target output is the high resolution version of the source image.</li></ul><figure><img src=/guided-super-resolution/guided-super-resolution_1.png alt=Trulli style=width:100%;align:center></figure><p>There are mainly 3 entities of the problem:</p><p>Guide image, source image and the target image.</p><p>Maybe you are asking what is the objective there? If we have the high-resolution guide why we are bothering ourselves to transform the source image. The objective relies on the domain difference. For example generally robots have two different cameras to construct vision with depth perception. First camera is a conventional camera that gathers RGB image, the second is time-of-flight camera or laser scanner which collects the depth information. But mostly the second camera has a very low resolution compared to the conventional camera. The objective is to transform this low-res image to a high-res image with the help of the guide image. We are trying to transfer high frequency details to the upsampled image of the source to obtain a sharp, high resolution version of it with the depth information.</p><h2 id=related-work>Related Work</h2><p>The starting point of Guided Super Resolution task is definitely bilateral filter. Bilateral Filter is a non-linear edge preserving filter that is widely used in image processing for noise reduction. The simple idea that relies behind this filter is the necessary information for a pixel value can be modeled as weighted average of the local neighborhood information.</p><figure><img src=/guided-super-resolution/guided-super-resolution_2.png alt=Trulli style=width:100%;align:center></figure><p>Bilateral filter is a conventional technique that gets a pixel and returns the weighted average of local neighborhood pixels. It is used for edge preserving, noise reduction and also for super-resolution. Low-resolution image upsampled to a higher one and than this image goes through Bilateral Filter to get smoother. Actually, Bilateral Filter is generalized version of the Guided Filters.</p><figure><img src=/guided-super-resolution/guided-super-resolution_3.png alt=Trulli style=width:100%;align:center></figure><p>Guided Filter is a framework that gets two images -one as a guide and the other one is the source- and returns the combination of the weighted averages for that pixel in both source and the guide image. It is used also for denoising and colorisation.</p><p>In the paper we dealt with not Guided Filter but Guided Super-Resolution task. It is the same as Guided Filters except we have the source at a lower resolution than the guide. Guided Super Resolution task extends the problem by adding super resolution which means transforming the source from lower to a higher resolution. This methods could be divided into two parts, first is local methods as for the Bilateral and Guided Filters, second is the global methods that formulate upsampling task as a global energy minimization.</p><p>The local methods consist of two step, first upsampling the source image to the higher resolution, then enhancing by the high-res guide. Global methods, instead trying to find a function that gets the source image and converts it to a high resolution target as good as possible. We define a loss function that aligned with the source, and trying to minimize this loss during the process.</p><figure><img src=/guided-super-resolution/guided-super-resolution_4.png alt=Trulli style=width:100%;align:center></figure><p>The Classical Approach to this task to formulate the task as an inverse problem: The source is understood as the result of downsampling the target image and the objective is to undo that operation by utilising the guide image to constrain the solution.</p><figure><img src=/guided-super-resolution/guided-super-resolution_5.png alt=Trulli style=width:100%;align:center></figure>The proposed approach is a little different from the Classical one. The role of the source and the guide are swapped. It formulates the problem as the transformation. of the guide to the target by constraining the output demanding its downsampled version matches the source image. It comes 2 practical advantages with this formulation : first by starting at the desired resolution and do not using the neighborhood information, input location do not mix, and secondly, by using the same mapping function for all pixels in the image, and regularize just the mapping function parameters, it results to sharper and crisp images.<h2 id=architecture-and-implementation>Architecture and Implementation</h2><figure><img src=/guided-super-resolution/guided-super-resolution_6.png alt=Trulli style=width:100%;align:center></figure><p>We have three different images : S(MxM) is the source, G(NxN) is the guide, and T(NxN) is the target image. Because the source is at a lower resolution, we have mapped DxD pixels of the guide or target to one pixel of the source. While mapping we are averaging the pixel value of the DxD pixels and these group of the pixels are denoted by b(m).</p><p>Now we are getting to the most important part of the paper: creating the loss function step by step. First, lets set the problem and the objective:</p><p>We can formulate the problem as : Finding a function that</p><figure><img src=/guided-super-resolution/guided-super-resolution_7.png alt=Trulli style=width:100%;align:center></figure><figure><img src=/guided-super-resolution/guided-super-resolution_8.png alt=Trulli style=width:100%;align:center></figure><p>gets the whole guide pixels and returns the target pixels where target pixels are consistent with the source.</p><p>As the first step of the Loss function we can start by :</p><figure><img src=/guided-super-resolution/guided-super-resolution_9.png alt=Trulli style=width:100%;align:center></figure><p>But there is a problem with that loss function, it is obviously ill-posed because we are downsampling the target function by averaging the DxD pixels, and there will be many solutions for that problem. Additionally, as indicated in the paper, for a given S, T, and G a perfect solution could be found by choosing a sufficiently complex function. So, we can solve this issue by regularizing with the l2 regularization our parameters. Our loss function will be :</p><figure><img src=/guided-super-resolution/guided-super-resolution_10.png alt=Trulli style=width:100%;align:center></figure><p>When we are looking this loss, we can easily see there is one-to-one relationships regardless of the locations. So, we can give the location information of the pixel as an input, by doing this we allow the function to take into account the location at the mapping stage.</p><figure><img src=/guided-super-resolution/guided-super-resolution_11.png alt=Trulli style=width:100%;align:center></figure><p>Now as a last problem, we are pushing not only guide pixel function, but also spatial function to be simple functions because of the same amount of regularization λ. We can separate the architecture into two parts: one for the pixel information and the other is the spatial information then combining them again.</p><figure><img src=/guided-super-resolution/guided-super-resolution_12.png alt=Trulli style=width:100%;align:center></figure><p>By this architecture, we have 3 different regularization coefficients: λg, λx and λhead. By this architecture we can obtain sharp and crisp target images.</p><p>For optimisation, classical stochastic gradient descent could be used. The architecture can be implemented by Convolutional neural nets by any deep learning framework. Author’s implementation with PyTorch could be found at <a href=https://github.com/prs-eth/PixTransform>https://github.com/prs-eth/PixTransform</a>.</p><p>I will finish by an important remark for the method. The method is unsupervised, which means you do not need other images than just Source, Guide and Target images. But you cannot use the same function for a different image. For every new image you should train the parameters again. One may ask, where is the neural network then, if we cannot use it for other cases? Actually the tricky part is here, the trained network generalize not the images, but pixels, we obtain a generalized function for every pixel of the image. As the title of the paper shows : it is Pixel-to-Pixel transformation.</p><p>For the results, I have run the code provided by the authors:</p><figure><img src=/guided-super-resolution/guided-super-resolution_13.png alt=Trulli style=width:100%;align:center></figure><p>The results of the experiment could be summarized as:</p><ul><li>Paper results are reproduced, because it is an unsupervised technique we can use it for other datasets</li><li>Because training and prediction must be done in the same stage, it becomes slow for inference. Thus it cannot be used for a streaming process.</li><li>For the high upsampling factors like 32x, the training is much longer and produced results are not quite good.</li><li>In 2022, there is proposed another solution by the same author as Learning Graph Regularization for Guided Super Resolution which does a better job with a bunch of data.</li></ul><p>Thank you for reading!!</p></section><nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]"><a class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://mehmetsat.github.io/homepage/neural-radiance-fields/><span class=mr-1.5>←</span><span>Neural Radiance Fields A Technical Deep Dive into 3D Scene Representation</span></a>
<a class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://mehmetsat.github.io/homepage/data-wrangling-with-pyspark/><span>Data Wrangling with PySpark</span><span class=ml-1.5>→</span></a></nav></article></main><footer class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"><div class=mr-auto>&copy; 2025
<a class=link href=https://mehmetsat.github.io/>Mehmet Sat | Personal Blog</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>Powered by Hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>✎ Paper</a></footer></body></html>