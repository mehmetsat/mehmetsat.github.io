<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<title>Data Wrangling with PySpark - Mehmet Sat | Personal Blog</title>
<meta name=description content="It becomes very hard to work with pandas when the data is huge. Instead as a strong alternative PySpark comes along. In this tutorial I will introduce the PySpark and mirror our pandas abilities to PySpark. This post assumes the reader has familiarity with the pandas library.
Introduction to Spark The first question is what is Spark. Apache Spark is fast and general engine for large-scale data processing. It distributes computations among the CPU cores to do parallel processing.">
<meta name=author content="mehmet sat">
<link rel="preload stylesheet" as=style href=https://mehmetsat.github.io/app.min.css>
<link rel="preload stylesheet" as=style href=https://mehmetsat.github.io/an-old-hope.min.css>
<script defer src=https://mehmetsat.github.io/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=preload as=image href=https://mehmetsat.github.io/theme.png>
<link rel=icon href=https://mehmetsat.github.io/favicon.ico>
<link rel=apple-touch-icon href=https://mehmetsat.github.io/apple-touch-icon.png>
<meta name=generator content="Hugo 0.91.2">
<meta property="og:title" content="Data Wrangling with PySpark">
<meta property="og:description" content="Data Wrangling with PySpark">
<meta property="og:type" content="article">
<meta property="og:url" content="https://mehmetsat.github.io/homepage/data-wrangling-with-pyspark/"><meta property="article:section" content="homepage">
<meta property="article:published_time" content="2021-08-17T00:00:00+00:00">
<meta property="article:modified_time" content="2021-08-17T00:00:00+00:00">
<meta itemprop=name content="Data Wrangling with PySpark">
<meta itemprop=description content="Data Wrangling with PySpark"><meta itemprop=datePublished content="2021-08-17T00:00:00+00:00">
<meta itemprop=dateModified content="2021-08-17T00:00:00+00:00">
<meta itemprop=wordCount content="898">
<meta itemprop=keywords content>
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Data Wrangling with PySpark">
<meta name=twitter:description content="Data Wrangling with PySpark">
</head>
<body class=not-ready data-menu=true>
<header class=header>
<p class=logo>
<a class=site-name href=https://mehmetsat.github.io/>Mehmet Sat | Personal Blog</a><a class=btn-dark></a>
</p>
<script>let bodyClx=document.body.classList,btnDark=document.querySelector('.btn-dark'),sysDark=window.matchMedia('(prefers-color-scheme: dark)'),darkVal=localStorage.getItem('dark'),setDark=a=>{bodyClx[a?'add':'remove']('dark'),localStorage.setItem('dark',a?'yes':'no')};setDark(darkVal?darkVal==='yes':sysDark.matches),requestAnimationFrame(()=>bodyClx.remove('not-ready')),btnDark.addEventListener('click',()=>setDark(!bodyClx.contains('dark'))),sysDark.addEventListener('change',a=>setDark(a.matches))</script>
<nav class=menu>
<a href=/about/>About</a>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-title>
<p>
<time>Aug 17, 2021</time>
<span>mehmet sat</span>
</p>
<h1>Data Wrangling with PySpark</h1>
</header>
<section class=post-content><p>It becomes very hard to work with pandas when the data is huge. Instead as a strong alternative PySpark comes along. In this tutorial I will introduce the PySpark and mirror our pandas abilities to PySpark. This post assumes the reader has familiarity with the pandas library.</p>
<h2 id=introduction-to-spark>Introduction to Spark</h2>
<p>The first question is what is Spark. Apache Spark is fast and general engine for large-scale data processing. It distributes computations among the CPU cores to do parallel processing. It has two abstractions:</p>
<ul>
<li>
<p>RDD (distributed collection of objects)</p>
</li>
<li>
<p>Dataframe (distributed dataset of Tabular data)</p>
</li>
</ul>
<p>Also there is two important qualities of Spark that you should know:</p>
<ul>
<li>
<p>Objects are immutable</p>
<ul>
<li>
<p>Changes create new object references</p>
</li>
<li>
<p>old versions are unchanged</p>
</li>
</ul>
</li>
<li>
<p>Lazy Evaluation</p>
<ul>
<li>
<p>Compute does not happen until output is requested</p>
</li>
<li>
<p>So it can optimize the processing very well</p>
</li>
</ul>
</li>
</ul>
<p>Setting up the environment and PySpark
I have used Anaconda to install Pyspark.</p>
<ul>
<li>
<p>First, create a new virtual environment:
from Anaconda Navigator or from a terminal you can create a virtual environment.</p>
</li>
<li>
<p>Then open up a terminal from the virtual environment that we have created. Install Pyspark by conda:</p>
<p><code>conda install pyspark</code></p>
</li>
<li>
<p>You can check if it is installed by :</p>
<p><code>pyspark</code></p>
</li>
</ul>
<p>you should see an output like this:</p>
<figure>
<img src=/data-wrangling-pyspark/image2.png alt=Trulli style=width:100%;align:center>
</figure>
<ul>
<li>
<p>You can start coding at Spark Shell but we will use jupyter notebook instead:
to exit from SparkSession just write exit()</p>
</li>
<li>
<p>To install jupyter notebook for our new virtual environment:</p>
<p><code>conda install jupyter</code></p>
</li>
<li>
<p>to open jupyter notebook:</p>
<p><code>jupyter notebook</code></p>
</li>
<li>
<p>Then we can test by opening a new notebook in our virtual environment and importing:</p>
<p>from pyspark import SparkContext
sc = SparkContext()
sc</p>
</li>
</ul>
<p>SparkContext is a package for starting a spark session. We are creating a spark object by calling <em>SparkContext()</em>
The output will be like this:</p>
<figure>
<img src=/data-wrangling-pyspark/image3.png alt=Trulli style=width:100%;align:center>
</figure>
<p>You can click to Spark UI to see the spark interface and jobs that you run.</p>
<p>Let’s create a RDD and track the job at the Spark UI. To create RDD :</p>
<pre><code>rdd = sc.parallelize(range(1000000000))
</code></pre>
<p>We create 1 billion range RDD. RDD is an immutable distributed collection of elements of your data. We create a count job for the RDD :</p>
<pre><code>rdd.count()
</code></pre>
<figure>
<img src=/data-wrangling-pyspark/image4.png alt=Trulli style=width:100%;align:center>
</figure>
<p>Track the details of the job at Spark UI:</p>
<figure>
<img src=/data-wrangling-pyspark/image5.png alt=Trulli style=width:100%;align:center>
</figure>
<p>As it can be seen above our job is distributed among 12 different cores of our CPU. It lasts 8 secs to be succeeded. We can also see the tasks in detail for every core:</p>
<figure>
<img src=/data-wrangling-pyspark/image6.png alt=Trulli style=width:100%;align:center>
</figure>
<p>Our set up is done. Now we can look at data wrangling with pyspark.</p>
<h2 id=data-wrangling-with-pyspark-for-who-knows-pandas>Data Wrangling with Pyspark for who knows Pandas</h2>
<p>First, we should initialize our SparkContext :</p>
<pre><code>from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
sc = SparkContext('local')
spark = SparkSession(sc)
</code></pre>
<h2 id=pandas-vs-pyspark>PANDAS vs PYSPARK</h2>
<p>For reading a csv file :</p>
<pre><code>#pandas
df = pd.read_csv('file_path')
#pyspark
df = spark.read.options(header=True,inferschema=True).csv(&quot;cf_data.csv&quot;)
</code></pre>
<p>To display a dataframe :</p>
<pre><code>#pandas
df
#pyspark
df.show()
</code></pre>
<p>But the output will be quite ugly compared to the pandas :</p>
<figure>
<img src=/data-wrangling-pyspark/image7.png alt=Trulli style=width:100%;align:center>
</figure>
<p>By default first 20 rows are displayed. To display n rows df.show(n) can be used.</p>
<p>To drop a column:</p>
<pre><code>#pandas 
df.drop('col_name')
#pyspark
df2.drop('rating').show()
</code></pre>
<figure>
<img src=/data-wrangling-pyspark/image8.png alt=Trulli style=width:100%;align:center>
</figure>
<p>Grouping is the same as pandas:</p>
<pre><code>df.groupby(['user','rating']).agg({'rating':'count'}) \
.sort('count(rating)', ascending=False).show()
</code></pre>
<p>we have sorted the items descendingly. By default aggregated columns are named like &lsquo;agg_func(col_name)&rsquo; which we used as a new column name for sorting :</p>
<figure>
<img src=/data-wrangling-pyspark/image9.png alt=Trulli style=width:100%;align:center>
</figure>
<p>Filtering the data is the same as pandas :</p>
<pre><code>df[df['rating']&gt;3].show()

df[(df['rating']&gt;3) &amp; (df['user']&gt;100)]
</code></pre>
<p>To add a column :</p>
<pre><code>#pandas 
df['inverse_rating'] = 1 / df['rating']
#pyspark
df = df.withColumn('inverse_rating', 1 / df.rating)
df.show()
</code></pre>
<p>To fill the nan values same as pandas :</p>
<pre><code>df = df.fillna(0)
</code></pre>
<p>For standart transformations we must import the pyspark.sql package to use built in functions :</p>
<pre><code>import pyspark.sql.functions as F
</code></pre>
<p>For example to use log function :</p>
<pre><code>#pandas
import numpy as np
df['log_rating'] = np.log(df.rating)
#pyspark
df = df.withColumn('log_rating',F.log(df.rating))
</code></pre>
<p>Although they seem pretty much the same, spark does not execute any python on the executors(lazy evaluation) so it is much faster compared to the pandas.</p>
<p>For row conditional statements :</p>
<pre><code>#pandas
df['Evaluation'] = df.rating.apply(lambda x: 'excellent' if x &gt; 4 else \
'good' if (x&gt;=3)&amp;(x&lt;=4) else 'bad')
#pyspark
df = df.withColumn('Evaluation',F.when(df.rating &gt; 4 , 'excellent') \
            .when((df.rating&gt;=3)&amp;(df.rating &lt;=4) , 'good') \
            .otherwise('bad'))
</code></pre>
<figure>
<img src=/data-wrangling-pyspark/image10.png alt=Trulli style=width:100%;align:center>
</figure>
<p>Other than built in functions we can define user-defined-function called udf:</p>
<pre><code>from pyspark.sql.types import DoubleType
fn = F.udf(lambda x: x+1 , DoubleType())
df = df.withColumn('rating+1',fn(df.rating))
df2 = df[['user','rating+1']]
df2.show()
</code></pre>
<figure>
<img src=/data-wrangling-pyspark/image11.png alt=Trulli style=width:100%;align:center>
</figure>
<p>But we should be careful about the return types. The function must be deterministic otherwise spark return nulls for every row. Here we have imported and used DoubleType.</p>
<p>To merge dataframes :</p>
<pre><code>#pandas
df.merge(df2,on='rating')
df.merge(df2,left_on = 'rating',right_on='rating')
#pyspark
df.join(df2,on='rating')
df.join(df2,df.rating==df2.rating)
</code></pre>
<figure>
<img src=/data-wrangling-pyspark/image12.png alt=Trulli style=width:100%;align:center>
</figure>
<p>Sometimes you really need a pandas abilities that does not exist on pyspark then:<br>
For example you need to plot a histogram, you can sample your data convert it to a pandas dataframe and then plot it in pandas:</p>
<pre><code>#pyspark
import matplotlib
%matplotlib inline 
df.sample(False,0.1).toPandas().plot()
</code></pre>
<p>Additionally, pyspark has SQL support which is not the case for pandas. That means you can write sql queries on your dataframe:</p>
<pre><code>df.createOrReplaceTempView('foo')
df2 = spark.sql('select * from foo')
df2.show()
</code></pre>
<figure>
<img src=/data-wrangling-pyspark/image13.png alt=Trulli style=width:100%;align:center>
</figure>
<p>That’s great. You can switch back and forth between views and dataframes. All the SQL functions and abilities can be used here.</p>
<h2 id=things-not-to-do>Things not To Do</h2>
<p>Do not try to iterate all the rows</p>
<pre><code>#Wrong
df.toPandas().head(5)
#First filter then convert
df.head(5).toPandas()
</code></pre>
<p>Thank you for reading!!</p>
<h3 id=references>References</h3>
<p>[1] <a href="https://www.youtube.com/watch?v=XrpSRCwISdk&t=1488s">Data Wrangling with PySpark for Data Scientists Who Know Pandas - Andrew Ray</a></p>
<p>[2] <a href="https://www.youtube.com/watch?v=MLXOy-OhWRY">How to install PySpark locally and use it with Jupyter Notebook 2021</a></p>
</section>
<nav class=post-nav>
<a class=next href=https://mehmetsat.github.io/homepage/recommendation-engines-overview/><span>Recommendation Engines Overview</span><span>→</span></a>
</nav>
</article>
</main>
<footer class=footer>
<p>&copy; 2021 <a href=https://mehmetsat.github.io/>Mehmet Sat | Personal Blog</a></p>
<p>Powered by <a href=https://gohugo.io/ rel=noopener target=_blank>Hugo️️</a>️</p>
<p>
<a href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>Paper 5.1</a>
</p>
</footer>
</body>
</html>