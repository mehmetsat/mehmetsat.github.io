<!doctype html><html class="not-ready lg:text-base" style=--bg:#faf8f1 lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Data Wrangling with PySpark - Mehmet Sat | Personal Blog</title><meta name=theme-color><meta name=description content="It becomes very hard to work with pandas when the data is huge. Instead as a strong alternative PySpark comes along. In this tutorial I will introduce the PySpark and mirror our pandas abilities to PySpark. This post assumes the reader has familiarity with the pandas library.
Introduction to Spark The first question is what is Spark. Apache Spark is fast and general engine for large-scale data processing. It distributes computations among the CPU cores to do parallel processing."><meta name=author content="mehmet sat"><link rel="preload stylesheet" as=style href=https://mehmetsat.github.io/main.min.css><link rel=preload as=image href=https://mehmetsat.github.io/theme.png><script defer src=https://mehmetsat.github.io/highlight.min.js onload=hljs.initHighlightingOnLoad()></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",()=>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1}))</script><link rel=icon href=https://mehmetsat.github.io/favicon.ico><link rel=apple-touch-icon href=https://mehmetsat.github.io/apple-touch-icon.png><meta name=generator content="Hugo 0.119.0"><meta itemprop=name content="Data Wrangling with PySpark"><meta itemprop=description content="Data Wrangling with PySpark"><meta itemprop=datePublished content="2021-08-17T00:00:00+00:00"><meta itemprop=dateModified content="2021-08-17T00:00:00+00:00"><meta itemprop=wordCount content="898"><meta itemprop=keywords content><meta property="og:title" content="Data Wrangling with PySpark"><meta property="og:description" content="Data Wrangling with PySpark"><meta property="og:type" content="article"><meta property="og:url" content="https://mehmetsat.github.io/homepage/data-wrangling-with-pyspark/"><meta property="article:section" content="homepage"><meta property="article:published_time" content="2021-08-17T00:00:00+00:00"><meta property="article:modified_time" content="2021-08-17T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Data Wrangling with PySpark"><meta name=twitter:description content="Data Wrangling with PySpark"><link rel=canonical href=https://mehmetsat.github.io/homepage/data-wrangling-with-pyspark/></head><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center"><div class="relative z-50 mr-auto flex items-center"><a class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold" href=https://mehmetsat.github.io/>Mehmet Sat | Personal Blog</a><div class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#faf8f1".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6"><a class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal" href=/about/>About</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"><article><header class=mb-16><h1 class="!my-0 pb-2.5">Data Wrangling with PySpark</h1><div class="text-sm antialiased opacity-60"><time>Aug 17, 2021</time>
<span class=mx-1>&#183;</span>
<span>mehmet sat</span></div></header><section><p>It becomes very hard to work with pandas when the data is huge. Instead as a strong alternative PySpark comes along. In this tutorial I will introduce the PySpark and mirror our pandas abilities to PySpark. This post assumes the reader has familiarity with the pandas library.</p><h2 id=introduction-to-spark>Introduction to Spark</h2><p>The first question is what is Spark. Apache Spark is fast and general engine for large-scale data processing. It distributes computations among the CPU cores to do parallel processing. It has two abstractions:</p><ul><li><p>RDD (distributed collection of objects)</p></li><li><p>Dataframe (distributed dataset of Tabular data)</p></li></ul><p>Also there is two important qualities of Spark that you should know:</p><ul><li><p>Objects are immutable</p><ul><li><p>Changes create new object references</p></li><li><p>old versions are unchanged</p></li></ul></li><li><p>Lazy Evaluation</p><ul><li><p>Compute does not happen until output is requested</p></li><li><p>So it can optimize the processing very well</p></li></ul></li></ul><p>Setting up the environment and PySpark
I have used Anaconda to install Pyspark.</p><ul><li><p>First, create a new virtual environment:
from Anaconda Navigator or from a terminal you can create a virtual environment.</p></li><li><p>Then open up a terminal from the virtual environment that we have created. Install Pyspark by conda:</p><p><code>conda install pyspark</code></p></li><li><p>You can check if it is installed by :</p><p><code>pyspark</code></p></li></ul><p>you should see an output like this:</p><figure><img src=/data-wrangling-pyspark/image2.png alt=Trulli style=width:100%;align:center></figure><ul><li><p>You can start coding at Spark Shell but we will use jupyter notebook instead:
to exit from SparkSession just write exit()</p></li><li><p>To install jupyter notebook for our new virtual environment:</p><p><code>conda install jupyter</code></p></li><li><p>to open jupyter notebook:</p><p><code>jupyter notebook</code></p></li><li><p>Then we can test by opening a new notebook in our virtual environment and importing:</p><p>from pyspark import SparkContext
sc = SparkContext()
sc</p></li></ul><p>SparkContext is a package for starting a spark session. We are creating a spark object by calling <em>SparkContext()</em>
The output will be like this:</p><figure><img src=/data-wrangling-pyspark/image3.png alt=Trulli style=width:100%;align:center></figure><p>You can click to Spark UI to see the spark interface and jobs that you run.</p><p>Let’s create a RDD and track the job at the Spark UI. To create RDD :</p><pre><code>rdd = sc.parallelize(range(1000000000))
</code></pre><p>We create 1 billion range RDD. RDD is an immutable distributed collection of elements of your data. We create a count job for the RDD :</p><pre><code>rdd.count()
</code></pre><figure><img src=/data-wrangling-pyspark/image4.png alt=Trulli style=width:100%;align:center></figure><p>Track the details of the job at Spark UI:</p><figure><img src=/data-wrangling-pyspark/image5.png alt=Trulli style=width:100%;align:center></figure><p>As it can be seen above our job is distributed among 12 different cores of our CPU. It lasts 8 secs to be succeeded. We can also see the tasks in detail for every core:</p><figure><img src=/data-wrangling-pyspark/image6.png alt=Trulli style=width:100%;align:center></figure><p>Our set up is done. Now we can look at data wrangling with pyspark.</p><h2 id=data-wrangling-with-pyspark-for-who-knows-pandas>Data Wrangling with Pyspark for who knows Pandas</h2><p>First, we should initialize our SparkContext :</p><pre><code>from pyspark.context import SparkContext
from pyspark.sql.session import SparkSession
sc = SparkContext('local')
spark = SparkSession(sc)
</code></pre><h2 id=pandas-vs-pyspark>PANDAS vs PYSPARK</h2><p>For reading a csv file :</p><pre><code>#pandas
df = pd.read_csv('file_path')
#pyspark
df = spark.read.options(header=True,inferschema=True).csv(&quot;cf_data.csv&quot;)
</code></pre><p>To display a dataframe :</p><pre><code>#pandas
df
#pyspark
df.show()
</code></pre><p>But the output will be quite ugly compared to the pandas :</p><figure><img src=/data-wrangling-pyspark/image7.png alt=Trulli style=width:100%;align:center></figure><p>By default first 20 rows are displayed. To display n rows df.show(n) can be used.</p><p>To drop a column:</p><pre><code>#pandas 
df.drop('col_name')
#pyspark
df2.drop('rating').show()
</code></pre><figure><img src=/data-wrangling-pyspark/image8.png alt=Trulli style=width:100%;align:center></figure><p>Grouping is the same as pandas:</p><pre><code>df.groupby(['user','rating']).agg({'rating':'count'}) \
.sort('count(rating)', ascending=False).show()
</code></pre><p>we have sorted the items descendingly. By default aggregated columns are named like &lsquo;agg_func(col_name)&rsquo; which we used as a new column name for sorting :</p><figure><img src=/data-wrangling-pyspark/image9.png alt=Trulli style=width:100%;align:center></figure><p>Filtering the data is the same as pandas :</p><pre><code>df[df['rating']&gt;3].show()

df[(df['rating']&gt;3) &amp; (df['user']&gt;100)]
</code></pre><p>To add a column :</p><pre><code>#pandas 
df['inverse_rating'] = 1 / df['rating']
#pyspark
df = df.withColumn('inverse_rating', 1 / df.rating)
df.show()
</code></pre><p>To fill the nan values same as pandas :</p><pre><code>df = df.fillna(0)
</code></pre><p>For standart transformations we must import the pyspark.sql package to use built in functions :</p><pre><code>import pyspark.sql.functions as F
</code></pre><p>For example to use log function :</p><pre><code>#pandas
import numpy as np
df['log_rating'] = np.log(df.rating)
#pyspark
df = df.withColumn('log_rating',F.log(df.rating))
</code></pre><p>Although they seem pretty much the same, spark does not execute any python on the executors(lazy evaluation) so it is much faster compared to the pandas.</p><p>For row conditional statements :</p><pre><code>#pandas
df['Evaluation'] = df.rating.apply(lambda x: 'excellent' if x &gt; 4 else \
'good' if (x&gt;=3)&amp;(x&lt;=4) else 'bad')
#pyspark
df = df.withColumn('Evaluation',F.when(df.rating &gt; 4 , 'excellent') \
            .when((df.rating&gt;=3)&amp;(df.rating &lt;=4) , 'good') \
            .otherwise('bad'))
</code></pre><figure><img src=/data-wrangling-pyspark/image10.png alt=Trulli style=width:100%;align:center></figure><p>Other than built in functions we can define user-defined-function called udf:</p><pre><code>from pyspark.sql.types import DoubleType
fn = F.udf(lambda x: x+1 , DoubleType())
df = df.withColumn('rating+1',fn(df.rating))
df2 = df[['user','rating+1']]
df2.show()
</code></pre><figure><img src=/data-wrangling-pyspark/image11.png alt=Trulli style=width:100%;align:center></figure><p>But we should be careful about the return types. The function must be deterministic otherwise spark return nulls for every row. Here we have imported and used DoubleType.</p><p>To merge dataframes :</p><pre><code>#pandas
df.merge(df2,on='rating')
df.merge(df2,left_on = 'rating',right_on='rating')
#pyspark
df.join(df2,on='rating')
df.join(df2,df.rating==df2.rating)
</code></pre><figure><img src=/data-wrangling-pyspark/image12.png alt=Trulli style=width:100%;align:center></figure><p>Sometimes you really need a pandas abilities that does not exist on pyspark then:<br>For example you need to plot a histogram, you can sample your data convert it to a pandas dataframe and then plot it in pandas:</p><pre><code>#pyspark
import matplotlib
%matplotlib inline 
df.sample(False,0.1).toPandas().plot()
</code></pre><p>Additionally, pyspark has SQL support which is not the case for pandas. That means you can write sql queries on your dataframe:</p><pre><code>df.createOrReplaceTempView('foo')
df2 = spark.sql('select * from foo')
df2.show()
</code></pre><figure><img src=/data-wrangling-pyspark/image13.png alt=Trulli style=width:100%;align:center></figure><p>That’s great. You can switch back and forth between views and dataframes. All the SQL functions and abilities can be used here.</p><h2 id=things-not-to-do>Things not To Do</h2><p>Do not try to iterate all the rows</p><pre><code>#Wrong
df.toPandas().head(5)
#First filter then convert
df.head(5).toPandas()
</code></pre><p>Thank you for reading!!</p><h3 id=references>References</h3><p>[1] <a href="https://www.youtube.com/watch?v=XrpSRCwISdk&amp;t=1488s">Data Wrangling with PySpark for Data Scientists Who Know Pandas - Andrew Ray</a></p><p>[2] <a href="https://www.youtube.com/watch?v=MLXOy-OhWRY">How to install PySpark locally and use it with Jupyter Notebook 2021</a></p></section><nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]"><a class="flex w-1/2 items-center rounded-l-md p-6 pr-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://mehmetsat.github.io/homepage/guided-super-resolution-as-pixel-to-pixel-transformation/><span class=mr-1.5>←</span><span>Guided Super Resolution as Pixel-to-Pixel Transformation</span></a>
<a class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://mehmetsat.github.io/homepage/recommendation-engines-overview/><span>Recommendation Engines Overview</span><span class=ml-1.5>→</span></a></nav></article></main><footer class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"><div class=mr-auto>&copy; 2025
<a class=link href=https://mehmetsat.github.io/>Mehmet Sat | Personal Blog</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>Powered by Hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>✎ Paper</a></footer></body></html>