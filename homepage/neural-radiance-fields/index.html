<!doctype html><html class="not-ready lg:text-base" style=--bg:#faf8f1 lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><title>Neural Radiance Fields A Technical Deep Dive into 3D Scene Representation - Mehmet Sat | Personal Blog</title>
<meta name=theme-color><meta name=description content="Neural Radiance Fields: A Technical Deep Dive into 3D Scene Neural Radiance Fields (NeRF) have revolutionized 3D scene representation and novel view synthesis by leveraging neural networks to model complex scenes with unprecedented detail and flexibility. Let&rsquo;s explore the technical foundations, key components, and underlying rationale of NeRF.
NeRF represents a 3D scene as a continuous 5D function:
$$ F: (x, y, z, \theta, \phi) \rightarrow (r, g, b, \sigma) $$"><meta name=author content="mehmet sat"><link rel="preload stylesheet" as=style href=https://mehmetsat.github.io/main.min.css><link rel=preload as=image href=https://mehmetsat.github.io/theme.png><script defer src=https://mehmetsat.github.io/highlight.min.js onload=hljs.initHighlightingOnLoad()></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",()=>renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1}))</script><link rel=icon href=https://mehmetsat.github.io/favicon.ico><link rel=apple-touch-icon href=https://mehmetsat.github.io/apple-touch-icon.png><meta name=generator content="Hugo 0.127.0"><meta itemprop=name content="Neural Radiance Fields A Technical Deep Dive into 3D Scene Representation"><meta itemprop=description content="Neural Radiance Fields A Technical Deep Dive into 3D Scene Representation"><meta itemprop=datePublished content="2024-06-21T00:00:00+00:00"><meta itemprop=dateModified content="2024-06-21T00:00:00+00:00"><meta itemprop=wordCount content="991"><meta property="og:url" content="https://mehmetsat.github.io/homepage/neural-radiance-fields/"><meta property="og:site_name" content="Mehmet Sat | Personal Blog"><meta property="og:title" content="Neural Radiance Fields A Technical Deep Dive into 3D Scene Representation"><meta property="og:description" content="Neural Radiance Fields A Technical Deep Dive into 3D Scene Representation"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="homepage"><meta property="article:published_time" content="2024-06-21T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-21T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Neural Radiance Fields A Technical Deep Dive into 3D Scene Representation"><meta name=twitter:description content="Neural Radiance Fields A Technical Deep Dive into 3D Scene Representation"><link rel=canonical href=https://mehmetsat.github.io/homepage/neural-radiance-fields/></head><body class="text-black duration-200 ease-out dark:text-white"><header class="mx-auto flex h-[4.5rem] max-w-3xl px-8 lg:justify-center"><div class="relative z-50 mr-auto flex items-center"><a class="-translate-x-[1px] -translate-y-[1px] text-2xl font-semibold" href=https://mehmetsat.github.io/>Mehmet Sat | Personal Blog</a><div class="btn-dark text-[0] ml-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]" role=button aria-label=Dark></div></div><div class="btn-menu relative z-50 -mr-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden" role=button aria-label=Menu></div><script>const htmlClass=document.documentElement.classList;setTimeout(()=>{htmlClass.remove("not-ready")},10);const btnMenu=document.querySelector(".btn-menu");btnMenu.addEventListener("click",()=>{htmlClass.toggle("open")});const metaTheme=document.querySelector('meta[name="theme-color"]'),lightBg="#faf8f1".replace(/"/g,""),setDark=e=>{metaTheme.setAttribute("content",e?"#000":lightBg),htmlClass[e?"add":"remove"]("dark"),localStorage.setItem("dark",e)},darkScheme=window.matchMedia("(prefers-color-scheme: dark)");if(htmlClass.contains("dark"))setDark(!0);else{const e=localStorage.getItem("dark");setDark(e?e==="true":darkScheme.matches)}darkScheme.addEventListener("change",e=>{setDark(e.matches)});const btnDark=document.querySelector(".btn-dark");btnDark.addEventListener("click",()=>{setDark(localStorage.getItem("dark")!=="true")})</script><div class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"><nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6"><a class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal" href=/about/>About</a></nav></div></header><main class="prose prose-neutral relative mx-auto min-h-[calc(100%-9rem)] max-w-3xl px-8 pb-16 pt-12 dark:prose-invert"><article><header class=mb-16><h1 class="!my-0 pb-2.5">Neural Radiance Fields A Technical Deep Dive into 3D Scene Representation</h1><div class="text-sm antialiased opacity-60"><time>Jun 21, 2024</time>
<span class=mx-1>&#183;</span>
<span>mehmet sat</span></div></header><section><h2 id=neural-radiance-fields-a-technical-deep-dive-into-3d-scene>Neural Radiance Fields: A Technical Deep Dive into 3D Scene</h2><p>Neural Radiance Fields (NeRF) have revolutionized 3D scene representation and novel view synthesis by leveraging neural networks to model complex scenes with unprecedented detail and flexibility. Let&rsquo;s explore the technical foundations, key components, and underlying rationale of NeRF.</p><p>NeRF represents a 3D scene as a continuous 5D function:</p><p>$$
F: (x, y, z, \theta, \phi) \rightarrow (r, g, b, \sigma)
$$</p><p>Where (x, y, z) represents spatial coordinates, (θ, φ) denotes viewing direction, and (r, g, b, σ) are the emitted color and volume density. This representation allows for smooth interpolation between observed views and handling of complex geometries and lighting effects.</p><p>To approximate this function, NeRF employs a Multi-Layer Perceptron (MLP) with typically 8-10 fully connected layers, each containing 256 or 512 neurons. The network architecture incorporates skip connections:</p><p>$$
h_l = \phi(W_l \cdot [h_{l-1}, \gamma(x)] + b_l)
$$</p><p>Where $h_l$ is the output of layer $l$, $W_l$ and $b_l$ are the weights and biases, $φ$ is the activation function (typically ReLU), and $γ(x)$ is the positional encoding of the input.</p><p>Skip connections are crucial because they allow the network to preserve fine-grained spatial information and learn both coarse structure and fine details effectively. By concatenating the input to the activations of intermediate layers, the network maintains a strong connection between the input coordinates and the final output, essential for accurately representing complex 3D environments.</p><p>A key innovation in NeRF is positional encoding:</p><p>$$
\gamma(p) = (\sin(2^0\pi p), \cos(2^0\pi p), \ldots, \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p))
$$</p><p>This encoding transforms input coordinates into a higher-dimensional space, enabling the network to learn high-frequency functions more effectively. The reason behind this is that neural networks often struggle to represent high-frequency details directly from raw input coordinates. By applying sine and cosine functions at different frequencies, positional encoding creates a unique &ldquo;fingerprint&rdquo; for each input value, separating nearby points in the high-dimensional space and making them easier for the network to distinguish.</p><p>The effectiveness of positional encoding lies in its multi-scale nature. Lower frequencies provide coarse positional information, while higher frequencies allow for encoding of finer details. Together, they create a rich, multi-scale representation of position, analogous to describing a location using maps at different scales.</p><p>Volume rendering is employed to generate 2D images from this 3D representation. The expected color C(r) of a camera ray r(t) = o + td is:</p><p>$$
C(r) = \int_{t_n}^{t_f} T(t) \sigma(r(t)) c(r(t), d) , dt
$$</p><p>Where
$$
T(t) = \exp\left(-\int_{t_n}^t \sigma(r(s)) , ds\right)
$$
is the accumulated transmittance.</p><p>In practice, this integral is approximated using quadrature:</p><p>$$
\tilde{C} = \sum_{i=1}^N T_i \left(1 - \exp(-\sigma_i \delta_i)\right) c_i
$$</p><p>Where $δ_i$ is the distance between adjacent samples and
$$
T_i = \exp\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)
$$</p><p>NeRF employs a hierarchical sampling strategy to efficiently allocate samples along each ray. This strategy is motivated by the need to balance computational efficiency with rendering quality. Two networks are used: a coarse network and a fine network. The coarse network samples $N_c$ points uniformly along the ray:</p><p>$$
t_i \sim U\left[t_n + \frac{(i-1)(t_f - t_n)}{N_c}, t_n + \frac{i(t_f - t_n)}{N_c}\right]
$$</p><p>The fine network then samples N_f points according to the piecewise-constant PDF:</p><p>$$
\hat{p}(t) = \frac{w_i}{\sum_{j=1}^{N_c} w_j} \quad \text{for } t \in [t_i, t_{i+1})
$$</p><p>Where
$$
w_i = T_i \left(1 - \exp(-\sigma_i \delta_i)\right)
$$</p><p>This two-stage approach allows NeRF to focus computational resources on the most relevant parts of the scene, improving rendering quality without dramatically increasing the total number of samples. The coarse network provides a rough estimate of the scene&rsquo;s structure, while the fine network refines the results in areas identified as important.</p><p>Training a NeRF model involves minimizing the mean squared error between the rendered and ground truth pixel colors:</p><p>$$
L = \sum_{r \in R} \left| \tilde{C}(r) - C(r) \right|^2_2
$$</p><p>Where R is the set of rays in the training images. The optimization process typically uses the Adam optimizer with an initial learning rate of 5e-4, which decays exponentially over the course of training.</p><p>A critical aspect of the training process is the differentiability of the entire pipeline. The coarse network&rsquo;s outputs need to be differentiable with respect to the fine network&rsquo;s inputs because the sampling process for the fine network depends on the coarse network&rsquo;s predictions. This allows for end-to-end training of the entire NeRF model, enabling both networks to learn simultaneously and interdependently.</p><p>Novel view synthesis is achieved by defining new camera parameters and rendering the scene using the trained model. The quality of synthesized views is often evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), and Learned Perceptual Image Patch Similarity (LPIPS).</p><p>An important consideration in novel view synthesis is the relationship between the resolution of the training data and the rendered output. While NeRF&rsquo;s continuous scene representation allows for rendering at arbitrary resolutions, the quality of higher resolution renders is ultimately limited by the information content of the training data. Rendering at higher resolutions than the training data might result in smoother images, but it won&rsquo;t reveal true details that weren&rsquo;t present in the original data.</p><p>While NeRF produces impressive results, it faces challenges in computational efficiency and handling of complex scene dynamics. The rendering process can be slow, especially for high-resolution images, due to the need to query the neural network multiple times for each ray. Additionally, the original NeRF model assumes static scenes, which limits its applicability to dynamic environments.</p><p>Current research focuses on addressing these limitations, with variants exploring real-time rendering, dynamic scene modeling, and improved sampling strategies. These advancements aim to broaden the applicability of NeRF to more diverse and complex scenarios.</p><p>In conclusion, Neural Radiance Fields represent a powerful approach to 3D scene representation, combining insights from computer graphics with deep learning techniques. By understanding the underlying principles and motivations behind each component of NeRF, we can appreciate its strengths and limitations, and anticipate future developments in this rapidly evolving field.​​​​​​​​​​​​​​​​</p></section><nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]"><a class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 font-semibold no-underline hover:bg-black/[2%] dark:hover:bg-white/[3%]" href=https://mehmetsat.github.io/homepage/aws-notes-vpc/><span>Virtual Private Clouds(VPC)</span><span class=ml-1.5>→</span></a></nav></article></main><footer class="opaco mx-auto flex h-[4.5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"><div class=mr-auto>&copy; 2024
<a class=link href=https://mehmetsat.github.io/>Mehmet Sat | Personal Blog</a></div><a class="link mx-6" href=https://gohugo.io/ rel=noopener target=_blank>Powered by Hugo️️</a>️
<a class=link href=https://github.com/nanxiaobei/hugo-paper rel=noopener target=_blank>✎ Paper</a></footer></body></html>